{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run LASSOPCR Searchlight\n",
    "\n",
    "\n",
    "*Yiyu Wang 2021 December*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# time stamp\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# for parallel processing\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# stats\n",
    "from scipy import linalg, ndimage, stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "#scikit learn\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "#cv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "# nifti handling\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import decoding\n",
    "import nilearn.masking as masking\n",
    "\n",
    "# from nilearn.glm import threshold_stats_img\n",
    "from nilearn import image\n",
    "from nilearn.image import new_img_like, load_img, get_data, concat_imgs,threshold_img\n",
    "\n",
    "\n",
    "# searchlihgt\n",
    "from nilearn.input_data.nifti_spheres_masker import _apply_mask_and_get_affinity\n",
    "from nilearn.image.resampling import coord_transform\n",
    "\n",
    "# plotting modules\n",
    "from nilearn import plotting\n",
    "from nilearn.plotting import plot_stat_map, plot_img, show\n",
    "from nilearn.plotting import plot_roi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code for submitting jobs on a cluster to run each model in parallel jobs\n",
    "import sys\n",
    "\n",
    "# array_id_position = 1\n",
    "# job_array_id = sys.argv[array_id_position]\n",
    "# print (\"Job_array_id %i: %s\" % (array_id_position, job_array_id))\n",
    "# model_index = int(job_array_id)\n",
    "\n",
    "# if no job array id, set model_index:\n",
    "model_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects in this analysis:\n",
      "['04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '23', '25', '26', '28', '29']\n",
      "**** n = 21 *****\n"
     ]
    }
   ],
   "source": [
    "# directories\n",
    "glm_dir = 'results/OneRegPerVid_VisReg/1stLvl/'\n",
    "res_dir = 'results/searchlight_lassopcr/'\n",
    "       \n",
    "if not os.path.isdir(res_dir):\n",
    "        os.mkdir(res_dir)\n",
    "        \n",
    "# masks:\n",
    "mask_path ='masks/FSL_binary_MNI152_T1_3mm_brain.nii.gz'\n",
    "mask = nib.load(mask_path)\n",
    "\n",
    "\n",
    "#load behavioral data\n",
    "behavdata_dir = 'BehavData/'\n",
    "zratings = glob.glob(behavdata_dir +'AffVids_novel_interpolated_rating_zscored.csv')\n",
    "zratings = pd.read_csv(zratings[0],index_col=0).reset_index()\n",
    "zratings = zratings.sort_values(by=['sub_id','run']).drop(['index'], axis =1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# subjects information\n",
    "subjects_str = ['04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','23','25','26','28','29'] \n",
    "subjects = list(range(4,20))+[23,25,26,28,29]\n",
    "Nsub = len(subjects)\n",
    "print(\"subjects in this analysis:\")\n",
    "print(subjects_str)\n",
    "print(f\"**** n = {Nsub} *****\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "video_list = list(range(1,37))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_lists(vcat):\n",
    "    \n",
    "    if vcat == 'Heights':\n",
    "        videos = list(range(1,13))\n",
    "    elif vcat == 'Social':\n",
    "        videos = list(range(13,25))\n",
    "    elif vcat == 'Spiders':\n",
    "        videos = list(range(25,37))\n",
    "    else: # v_cat == 'Situation_General'\n",
    "        videos = list(range(1,37))\n",
    "\n",
    "    return videos\n",
    "\n",
    "def get_category(vn):\n",
    "    if vn < 13:\n",
    "        cat = 'Heights'\n",
    "        cn = 1\n",
    "    elif vn > 24:\n",
    "        cat = 'Spiders'\n",
    "        cn = 3\n",
    "    else:\n",
    "        cat = 'Social'\n",
    "        cn = 2\n",
    "    return [cat,cn]\n",
    "\n",
    "\n",
    "# copied from scipy : remove returning p value so that this function can be converted to a scorer in scikit-learn\n",
    "def pearsonr(x, y):\n",
    "    n = len(x)\n",
    "    if n != len(y):\n",
    "        raise ValueError('x and y must have the same length.')\n",
    "\n",
    "    if n < 2:\n",
    "        raise ValueError('x and y must have length at least 2.')\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if (x == x[0]).all() or (y == y[0]).all():\n",
    "        return np.nan\n",
    "\n",
    "    dtype = type(1.0 + x[0] + y[0])\n",
    "\n",
    "    if n == 2:\n",
    "        return dtype(np.sign(x[1] - x[0])*np.sign(y[1] - y[0]))\n",
    "\n",
    "    xmean = x.mean(dtype=dtype)\n",
    "    ymean = y.mean(dtype=dtype)\n",
    "\n",
    "    xm = x.astype(dtype) - xmean\n",
    "    ym = y.astype(dtype) - ymean\n",
    "\n",
    "    normxm = linalg.norm(xm)\n",
    "    normym = linalg.norm(ym)\n",
    "\n",
    "    threshold = 1e-13\n",
    "    if normxm < threshold*abs(xmean) or normym < threshold*abs(ymean):\n",
    "        print('values close to the mean')\n",
    "\n",
    "    r = np.dot(xm/normxm, ym/normym)\n",
    "    r = max(min(r, 1.0), -1.0)\n",
    "    return r\n",
    "\n",
    "\n",
    "class GroupIterator(object):\n",
    "    def __init__(self, n_features, n_jobs):\n",
    "        self.n_features = n_features\n",
    "        if n_jobs == -1:\n",
    "            n_jobs = cpu_count()\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def __iter__(self):\n",
    "        split = np.array_split(np.arange(self.n_features), self.n_jobs)\n",
    "        for list_i in split:\n",
    "            yield list_i\n",
    "\n",
    "\n",
    "\n",
    "# make lassopcr searchlight a function:\n",
    "def my_lassopcr_searchlight(list_i, list_rows,X_train,X_test,train_y, test_y, thread_id):\n",
    "\n",
    "    # check if the voxel index (list_i) is the same lenth as input data (list_rows)\n",
    "    if len(list_rows) != len(list_i):\n",
    "        raise ValueError('Voxel index does not equal to input data size!!!!')\n",
    "\n",
    "    sl_scores=np.zeros(len(list_rows))\n",
    "    sl_rmse = np.zeros(len(list_rows))\n",
    "    \n",
    "    # list_rows = A_train.rows[list_i]\n",
    "    # train_y = shuffled_train_y if running permutation\n",
    "    # test_y = shuffled_test_y if running permutation\n",
    "    for i, row in enumerate(list_rows):\n",
    "        \n",
    "        n_components=min(len(X_test[:, row]), len(X_train[:, row]))\n",
    "        pca_fit = decomposition.PCA(n_components = n_components) # n_comp = number of testing sample (smaller than training sample)\n",
    "        pca_train_x = pca_fit.fit_transform(X_train[:, row])\n",
    "\n",
    "        pca_test_x = pca_fit.transform(X_test[:, row])\n",
    "\n",
    "        # run LASSO\n",
    "        clf = linear_model.Lasso(alpha=alpha,max_iter=5000)\n",
    "        clf.fit(pca_train_x, train_y)\n",
    "\n",
    "        prediction_test =clf.predict(pca_test_x)\n",
    "\n",
    "        r_value = pearsonr(test_y, prediction_test)\n",
    "        sl_scores[i] =r_value\n",
    "\n",
    "        rmse = np.sqrt(np.mean((prediction_test-test_y)**2))\n",
    "        sl_rmse[i] = rmse\n",
    "    \n",
    "    sl_scores_combined = [list(a) for a in zip(sl_scores, sl_rmse)]\n",
    "    return sl_scores_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold split pre-generated:\n",
      "('Situation_General', 'Situation_General', 2)\n",
      "running model: train-Situation_General, test-Situation_General, 2 fold\n",
      "training Category:  Situation_General\n",
      "Testing Category:  Situation_General\n",
      "TRAIN Subjects: [4, 5, 6, 7, 11, 12, 13, 15, 16, 18, 19, 25, 26, 29]\n",
      "TEST Subjects: [8, 9, 10, 14, 17, 23, 28]\n"
     ]
    }
   ],
   "source": [
    "# model information\n",
    "my_radius = 15\n",
    "k_fold = 3\n",
    "n_jobs = 16\n",
    "alpha = 0.01 #LASSO regularization term\n",
    "\n",
    "print(f'kfold split pre-generated:')\n",
    "cv_train_subjects = [[4, 5, 6, 7, 8, 9, 10, 14, 17, 19, 23, 25, 28, 29],\n",
    " [4, 5, 6, 7, 11, 12, 13, 15, 16, 18, 19, 25, 26, 29],\n",
    " [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 26, 28]]\n",
    "\n",
    "cv_test_subjects = [[11, 12, 13, 15, 16, 18, 26],\n",
    " [8, 9, 10, 14, 17, 23, 28],\n",
    " [4, 5, 6, 7, 19, 25, 29]]\n",
    "\n",
    "# prepare cross validation:\n",
    "train_list = ['Situation_General','Heights','Social','Spiders']\n",
    "test_list = ['Situation_General','Heights','Social','Spiders']\n",
    "cv_list = [1,2,3]\n",
    "\n",
    "all_comb = list(itertools.product(train_list,test_list,cv_list))\n",
    "model = all_comb[model_index]\n",
    "print(model)\n",
    "\n",
    "train_cat = model[0]\n",
    "test_cat = model[1]\n",
    "cv_iter = model[2]\n",
    "\n",
    "print(f'running model: train-{train_cat}, test-{test_cat}, {cv_iter} fold')\n",
    "\n",
    "testing_videos = get_video_lists(test_cat)\n",
    "training_videos = get_video_lists(train_cat)\n",
    "print(\"training Category: \", train_cat)\n",
    "print(\"Testing Category: \", test_cat)\n",
    "\n",
    "\n",
    "train_subjects = cv_train_subjects[cv_iter-1]\n",
    "test_subjects = cv_test_subjects[cv_iter-1]\n",
    "print(\"TRAIN Subjects:\", train_subjects)\n",
    "print(\"TEST Subjects:\", test_subjects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject: 04\n",
      "running subject: 05\n",
      "running subject: 06\n",
      "running subject: 07\n",
      "running subject: 08\n",
      "running subject: 09\n",
      "running subject: 10\n",
      "running subject: 11\n",
      "running subject: 12\n",
      "running subject: 13\n",
      "running subject: 14\n",
      "running subject: 15\n",
      "running subject: 16\n",
      "running subject: 17\n",
      "running subject: 18\n",
      "running subject: 19\n",
      "running subject: 23\n",
      "running subject: 25\n",
      "running subject: 26\n",
      "running subject: 28\n",
      "running subject: 29\n"
     ]
    }
   ],
   "source": [
    "# load nifti data into data\n",
    "data = []\n",
    "nii_file_list = []\n",
    "nifti_masker = NiftiMasker(mask_img=mask)\n",
    "\n",
    "for sidx in range(len(subjects)):\n",
    "    s_str = subjects_str[sidx]\n",
    "    s = subjects[sidx]\n",
    "    print('running subject: ' + s_str)\n",
    "\n",
    "    for v in zratings[zratings.sub_id == s].video_number:\n",
    "        v = int(v)\n",
    "        filename =glm_dir + f'{s_str}/sub-{s_str}_run-*_beta_video-{v}_gm_visreg.nii.gz'\n",
    "        filename =glob.glob(filename)\n",
    "        if filename:\n",
    "            nii_file_list.append(filename[0])\n",
    "            #data.append(nib.load(filename[0]).get_fdata()[:,:,:,0])\n",
    "            data.append(nib.load(filename[0]).get_fdata())\n",
    "\n",
    "# so that we can use train_index and test_index in the forth dimension\n",
    "data = np.moveaxis(data, 0, -1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to searchlight space:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute searchlight coordinates from mask\n",
    "process_mask_img = mask\n",
    "\n",
    "process_mask, process_mask_affine = masking._load_mask_img(\n",
    "    process_mask_img)\n",
    "process_mask_coords = np.where(process_mask != 0)\n",
    "process_mask_coords = coord_transform(\n",
    "    process_mask_coords[0], process_mask_coords[1],\n",
    "    process_mask_coords[2], process_mask_affine)\n",
    "process_mask_coords = np.asarray(process_mask_coords).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prepare information for each model:   \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "iter_line = f\"iteration {cv_iter} starts at {dt_string} \\n\"\n",
    "print(iter_line)\n",
    "\n",
    "# get the index from the behav_data(zratings)\n",
    "train_index = list(zratings[zratings.sub_id.isin(train_subjects) & zratings.video_number.isin(training_videos)].index)\n",
    "test_index = list(zratings[zratings.sub_id.isin(test_subjects) & zratings.video_number.isin(testing_videos)].index)\n",
    "\n",
    "    \n",
    "# behavioral data (y)\n",
    "train_y = zratings.iloc[train_index].fear\n",
    "test_y = zratings.iloc[test_index].fear\n",
    "\n",
    "# brain data (x)\n",
    "train_x = data[:,:,:,train_index]\n",
    "test_x = data[:,:,:,test_index]\n",
    "    \n",
    "# make img objects for training and testing data to be converted to searchlight:\n",
    "new_affine = mask.affine.copy()\n",
    "\n",
    "train_img = new_img_like(mask, train_x, affine=new_affine)\n",
    "\n",
    "test_img = new_img_like(mask, test_x, affine=new_affine)\n",
    "\n",
    "\n",
    "# convert training and testing data to searchlight:\n",
    "# !!! This can take a long time!!\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"training: applying mask and get affinity: {dt_string}\")\n",
    "\n",
    "# make sure to pass in a mask - this saves a lot of time\n",
    "X_train, A_train = _apply_mask_and_get_affinity(\n",
    "            process_mask_coords, train_img, my_radius, True, mask_img=mask)\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"testing: applying mask and get affinity: {dt_string}\")\n",
    "\n",
    "\n",
    "X_test, A_test = _apply_mask_and_get_affinity(\n",
    "            process_mask_coords, test_img, my_radius, True, mask_img=mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LASSOPCR Searchlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting searchlight')\n",
    "this_cv_scores = []\n",
    "this_cv_rmse = []\n",
    "group_iter = GroupIterator(A_train.shape[0], n_jobs)\n",
    "with warnings.catch_warnings():  # might not converge\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    sl_scores_combined = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        delayed(my_lassopcr_searchlight)(\n",
    "            list_i,\n",
    "            A_train.rows[list_i],\n",
    "            X_train,\n",
    "            X_test,\n",
    "            train_y,\n",
    "            test_y, \n",
    "           thread_id)\n",
    "        for thread_id, list_i in enumerate(group_iter))\n",
    "\n",
    "sl_scores_combined = np.array(sl_scores_combined, dtype=object)\n",
    "\n",
    "sl_scores = sl_scores_combined[:,:,0]\n",
    "reshaped_sl_scores = np.reshape(sl_scores, (sl_scores.shape[0] * sl_scores.shape[1]))\n",
    "scores_3D = np.zeros(process_mask.shape)\n",
    "scores_3D[process_mask] = reshaped_sl_scores\n",
    "\n",
    "\n",
    "sl_rmse = sl_scores_combined[:,:,1]\n",
    "reshaped_sl_rmse = np.reshape(sl_rmse,(sl_rmse.shape[0]*sl_rmse.shape[1]))\n",
    "rmse_3D = np.zeros(process_mask.shape)\n",
    "rmse_3D[process_mask] = reshaped_sl_rmse\n",
    "\n",
    "# save the scores\n",
    "this_cv_scores.append(scores_3D)\n",
    "this_cv_rmse.append(rmse_3D)\n",
    "\n",
    "this_cv_scores = np.moveaxis(this_cv_scores, 0, -1)\n",
    "this_cv_rmse = np.moveaxis(this_cv_rmse, 0, -1)\n",
    "\n",
    "this_cv_affine = mask.affine\n",
    "this_cv_affine[3,3] = k_fold\n",
    "\n",
    "this_cv_scores_img = new_img_like(mask, this_cv_scores, affine=this_cv_affine)\n",
    "this_cv_rmse_img = new_img_like(mask, this_cv_rmse, affine=this_cv_affine)\n",
    "\n",
    "print('saving: iteration' + str(cv_iter))\n",
    "nib.save(this_cv_scores_img,res_dir + f'cv{cv_iter}_kfold3_searchlight_pearsonr_train_{train_cat}_test_{test_cat}_{job_id}-{job_array_id}.nii.gz')\n",
    "nib.save(this_cv_rmse_img,res_dir + f'cv{cv_iter}_kfold3_searchlight_rmse_train_{train_cat}_test_{test_cat}_{job_id}-{job_array_id}.nii.gz')\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "iter_line = f\"iteration {cv_iter} finished at {dt_string}\"\n",
    "print(iter_line)\n",
    "\n",
    "    \n",
    "# dd/mm/YY H:M:S\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"end time: \", dt_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
