{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Run Permutation:\n",
    "\n",
    "fix cross-validation fold such that the null distribution maintain subject variations. This is to ensure that the results do not exceed significance threshold in the permutation test because of higher across-subjects variations.\n",
    "\n",
    "*Yiyu Wang 2021 December*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time stamp\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for parallel processing\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# stats\n",
    "from scipy import linalg, ndimage, stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "#scikit learn\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import decomposition\n",
    "\n",
    "# nifti handling\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import decoding\n",
    "import nilearn.masking as masking\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn.image import new_img_like, load_img, get_data, concat_imgs,threshold_img\n",
    "\n",
    "# searchlihgt\n",
    "from nilearn.input_data.nifti_spheres_masker import _apply_mask_and_get_affinity\n",
    "from nilearn.image.resampling import coord_transform\n",
    "\n",
    "# plotting modules\n",
    "from nilearn import plotting\n",
    "from nilearn.plotting import plot_stat_map, plot_img, plot_roi, show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster job info:\n",
    "this is code for submitting jobs on a cluster to run permutation iteraations in parallel jobs\n",
    "because each job has 3 cv iterations, technically only needs (total n iteration / 3) as your job array id\n",
    "(e.g., #SBATCH --array=0-400 should produce about 1200 permutations)\n",
    "\n",
    "Need to manually change the train and test situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "array_id_position = 1\n",
    "job_array_id = sys.argv[array_id_position]\n",
    "print (\"Job_array_id %i: %s\" % (array_id_position, job_array_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects in this analysis:\n",
      "['04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '23', '25', '26', '28', '29']\n",
      "**** n = 21 *****\n"
     ]
    }
   ],
   "source": [
    "# directories\n",
    "glm_dir = 'results/OneRegPerVid_VisReg/1stLvl/'\n",
    "res_dir = 'results/permutation/'\n",
    "       \n",
    "if not os.path.isdir(res_dir):\n",
    "        os.mkdir(res_dir)\n",
    "        \n",
    "# masks:\n",
    "mask_path ='masks/FSL_binary_MNI152_T1_3mm_brain.nii.gz'\n",
    "mask = nib.load(mask_path)\n",
    "\n",
    "\n",
    "#load behavioral data\n",
    "behavdata_dir =  'BehavData/'\n",
    "\n",
    "zratings = glob.glob(behavdata_dir +'AffVids_novel_interpolated_rating_zscored.csv')\n",
    "zratings = pd.read_csv(zratings[0],index_col=0).reset_index()\n",
    "zratings = zratings.sort_values(by=['sub_id','run']).drop(['index'], axis =1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# subjects information\n",
    "subjects_str = ['04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','23','25','26','28','29'] \n",
    "subjects = list(range(4,20))+[23,25,26,28,29]\n",
    "Nsub = len(subjects)\n",
    "print(\"subjects in this analysis:\")\n",
    "print(subjects_str)\n",
    "print(f\"**** n = {Nsub} *****\" )\n",
    "\n",
    "\n",
    "video_list = list(range(1,37))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_lists(vcat):\n",
    "    \n",
    "    if vcat == 'Heights':\n",
    "        videos = list(range(1,13))\n",
    "    elif vcat == 'Social':\n",
    "        videos = list(range(13,25))\n",
    "    elif vcat == 'Spiders':\n",
    "        videos = list(range(25,37))\n",
    "    else: # v_cat == 'Situation_General'\n",
    "        videos = list(range(1,37))\n",
    "\n",
    "    return videos\n",
    "\n",
    "def get_category(vn):\n",
    "    if vn < 13:\n",
    "        cat = 'Heights'\n",
    "        cn = 1\n",
    "    elif vn > 24:\n",
    "        cat = 'Spiders'\n",
    "        cn = 3\n",
    "    else:\n",
    "        cat = 'Social'\n",
    "        cn = 2\n",
    "    return [cat,cn]\n",
    "\n",
    "\n",
    "# copied from scipy : remove returning p value so that this function can be converted to a scorer in scikit-learn\n",
    "def pearsonr(x, y):\n",
    "    n = len(x)\n",
    "    if n != len(y):\n",
    "        raise ValueError('x and y must have the same length.')\n",
    "\n",
    "    if n < 2:\n",
    "        raise ValueError('x and y must have length at least 2.')\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if (x == x[0]).all() or (y == y[0]).all():\n",
    "        return np.nan\n",
    "\n",
    "    dtype = type(1.0 + x[0] + y[0])\n",
    "\n",
    "    if n == 2:\n",
    "        return dtype(np.sign(x[1] - x[0])*np.sign(y[1] - y[0]))\n",
    "\n",
    "    xmean = x.mean(dtype=dtype)\n",
    "    ymean = y.mean(dtype=dtype)\n",
    "\n",
    "    xm = x.astype(dtype) - xmean\n",
    "    ym = y.astype(dtype) - ymean\n",
    "\n",
    "    normxm = linalg.norm(xm)\n",
    "    normym = linalg.norm(ym)\n",
    "\n",
    "    threshold = 1e-13\n",
    "    if normxm < threshold*abs(xmean) or normym < threshold*abs(ymean):\n",
    "        print('values close to the mean')\n",
    "\n",
    "    r = np.dot(xm/normxm, ym/normym)\n",
    "    r = max(min(r, 1.0), -1.0)\n",
    "    return r\n",
    "\n",
    "\n",
    "class GroupIterator(object):\n",
    "    def __init__(self, n_features, n_jobs):\n",
    "        self.n_features = n_features\n",
    "        if n_jobs == -1:\n",
    "            n_jobs = cpu_count()\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def __iter__(self):\n",
    "        split = np.array_split(np.arange(self.n_features), self.n_jobs)\n",
    "        for list_i in split:\n",
    "            yield list_i\n",
    "\n",
    "\n",
    "\n",
    "# make lassopcr searchlight a function:\n",
    "def my_lassopcr_searchlight(list_i, list_rows,X_train,X_test,train_y, test_y, thread_id):\n",
    "\n",
    "    # check if the voxel index (list_i) is the same lenth as input data (list_rows)\n",
    "    if len(list_rows) != len(list_i):\n",
    "        raise ValueError('Voxel index does not equal to input data size!!!!')\n",
    "\n",
    "    sl_scores=np.zeros(len(list_rows))\n",
    "    sl_rmse = np.zeros(len(list_rows))\n",
    "    \n",
    "    # list_rows = A_train.rows[list_i]\n",
    "    # train_y = shuffled_train_y if running permutation\n",
    "    # test_y = shuffled_test_y if running permutation\n",
    "    for i, row in enumerate(list_rows):\n",
    "        \n",
    "        n_components=min(len(X_test[:, row]), len(X_train[:, row]))\n",
    "        pca_fit = decomposition.PCA(n_components = n_components) # n_comp = number of testing sample (smaller than training sample)\n",
    "        pca_train_x = pca_fit.fit_transform(X_train[:, row])\n",
    "\n",
    "        pca_test_x = pca_fit.transform(X_test[:, row])\n",
    "\n",
    "        # run LASSO\n",
    "        clf = linear_model.Lasso(alpha=alpha,max_iter=5000)\n",
    "        clf.fit(pca_train_x, train_y)\n",
    "\n",
    "        prediction_test =clf.predict(pca_test_x)\n",
    "\n",
    "        r_value = pearsonr(test_y, prediction_test)\n",
    "        sl_scores[i] =r_value\n",
    "\n",
    "        rmse = np.sqrt(np.mean((prediction_test-test_y)**2))\n",
    "        sl_rmse[i] = rmse\n",
    "    \n",
    "    sl_scores_combined = [list(a) for a in zip(sl_scores, sl_rmse)]\n",
    "    return sl_scores_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold split pre-generated:\n"
     ]
    }
   ],
   "source": [
    "# model information\n",
    "my_radius = 15\n",
    "k_fold = 3\n",
    "n_jobs = 16\n",
    "alpha = 0.01 #LASSO regularization term\n",
    "\n",
    "print(f'kfold split pre-generated:')\n",
    "cv_train_subjects = [[4, 5, 6, 7, 8, 9, 10, 14, 17, 19, 23, 25, 28, 29],\n",
    " [4, 5, 6, 7, 11, 12, 13, 15, 16, 18, 19, 25, 26, 29],\n",
    " [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 26, 28]]\n",
    "\n",
    "cv_test_subjects = [[11, 12, 13, 15, 16, 18, 26],\n",
    " [8, 9, 10, 14, 17, 23, 28],\n",
    " [4, 5, 6, 7, 19, 25, 29]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject: 04\n",
      "running subject: 05\n",
      "running subject: 06\n",
      "running subject: 07\n",
      "running subject: 08\n",
      "running subject: 09\n",
      "running subject: 10\n",
      "running subject: 11\n",
      "running subject: 12\n",
      "running subject: 13\n",
      "running subject: 14\n",
      "running subject: 15\n",
      "running subject: 16\n",
      "running subject: 17\n",
      "running subject: 18\n",
      "running subject: 19\n",
      "running subject: 23\n",
      "running subject: 25\n",
      "running subject: 26\n",
      "running subject: 28\n",
      "running subject: 29\n"
     ]
    }
   ],
   "source": [
    "# load nifti data into data\n",
    "data = []\n",
    "nii_file_list = []\n",
    "nifti_masker = NiftiMasker(mask_img=mask)\n",
    "\n",
    "for sidx in range(len(subjects)):\n",
    "    s_str = subjects_str[sidx]\n",
    "    s = subjects[sidx]\n",
    "    print('running subject: ' + s_str)\n",
    "\n",
    "\n",
    "    for v in zratings[zratings.sub_id == s].video_number:\n",
    "        v = int(v)\n",
    "        filename =glm_dir + f'{s_str}/sub-{s_str}_run-*_beta_video-{v}_gm_visreg.nii.gz'\n",
    "        filename =glob.glob(filename)\n",
    "        if filename:\n",
    "            nii_file_list.append(filename[0])\n",
    "            #data.append(nib.load(filename[0]).get_fdata()[:,:,:,0])\n",
    "            data.append(nib.load(filename[0]).get_fdata())\n",
    "\n",
    "data = np.moveaxis(data, 0, -1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Category:  Situation_General\n",
      "Testing Category:  Heights\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# prepare cv:\n",
    "\n",
    "\n",
    "#############################################\n",
    "# change the category before submitting jobs\n",
    "train_cat = 'Situation_General'\n",
    "test_cat = 'Heights'\n",
    "#############################################\n",
    "\n",
    "print(\"training Category: \", train_cat)\n",
    "print(\"Testing Category: \", test_cat)\n",
    "\n",
    "\n",
    "testing_videos = get_video_lists(test_cat)\n",
    "training_videos = get_video_lists(train_cat)\n",
    "print(training_videos)\n",
    "print(testing_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute searchlight coordinates\n",
    "process_mask_img = mask\n",
    "process_mask, process_mask_affine = masking._load_mask_img(\n",
    "    process_mask_img)\n",
    "process_mask_coords = np.where(process_mask != 0)\n",
    "process_mask_coords = coord_transform(\n",
    "    process_mask_coords[0], process_mask_coords[1],\n",
    "    process_mask_coords[2], process_mask_affine)\n",
    "process_mask_coords = np.asarray(process_mask_coords).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run Searchlight + LASSOPCR:\n",
    "loop through all 3 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for cv_iter in range(1,k_fold+1):\n",
    "    train_subjects = cv_train_subjects[cv_iter-1]\n",
    "    test_subjects = cv_test_subjects[cv_iter-1]\n",
    "\n",
    "\n",
    "    train_index = list(zratings[zratings.sub_id.isin(train_subjects) & zratings.video_number.isin(training_videos)].index)\n",
    "    test_index = list(zratings[zratings.sub_id.isin(test_subjects) & zratings.video_number.isin(testing_videos)].index)\n",
    "    \n",
    "\n",
    "    # shuffle labels of training data:\n",
    "    shuffled_train_index = random.sample(train_index,len(train_index))\n",
    "    shuffled_train_x = data[:,:,:,shuffled_train_index]\n",
    "    train_y = zratings.iloc[train_index].fear\n",
    "    shuffled_train_y = train_y.sample(frac=1)\n",
    "\n",
    "\n",
    "    # shuffle labels of testing data:\n",
    "    shuffled_test_index = random.sample(test_index,len(test_index))\n",
    "    print(\"shuffled test_x index: \", shuffled_test_index)\n",
    "    shuffled_test_x = data[:,:,:,shuffled_test_index]\n",
    "    test_y = zratings.iloc[test_index].fear\n",
    "    shuffled_test_y = test_y.sample(frac=1)\n",
    "    print(\"shuffled test y: \", shuffled_test_y)\n",
    "\n",
    "    # make img objects for training and shuffled testing data\n",
    "    new_affine = mask.affine.copy()\n",
    "    new_affine[3,3] = np.shape(shuffled_train_x)[3]\n",
    "    train_img = new_img_like(mask, shuffled_train_x, affine=new_affine)\n",
    "\n",
    "    new_affine[3,3] = np.shape(shuffled_test_x)[3]\n",
    "    test_img = new_img_like(mask, shuffled_test_x, affine=new_affine)\n",
    "\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"training: applying mask and get affinity: \", dt_string)\n",
    "    X_train, A_train = _apply_mask_and_get_affinity(\n",
    "                process_mask_coords, train_img, my_radius, True, mask_img=None)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"testing: applying mask and get affinity: \", dt_string)\n",
    "\n",
    "\n",
    "    X_test, A_test = _apply_mask_and_get_affinity(\n",
    "                process_mask_coords, test_img, my_radius, True, mask_img=None)\n",
    "\n",
    "\n",
    "\n",
    "    this_cv_scores = []\n",
    "    this_cv_rmse = []\n",
    "\n",
    "    print('starting searchlight')\n",
    "    group_iter = GroupIterator(A_train.shape[0], n_jobs)\n",
    "    with warnings.catch_warnings():  # might not converge\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "        sl_scores_combined = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "            delayed(my_lassopcr_searchlight)(\n",
    "                list_i,\n",
    "                A_train.rows[list_i],\n",
    "                X_train,\n",
    "                X_test,\n",
    "                shuffled_train_y,\n",
    "                shuffled_test_y, \n",
    "                thread_id)\n",
    "            for thread_id, list_i in enumerate(group_iter))\n",
    "\n",
    "    sl_scores_combined = np.array(sl_scores_combined)\n",
    "\n",
    "    sl_scores = sl_scores_combined[:,:,0]\n",
    "    reshaped_sl_scores = np.reshape(sl_scores, (sl_scores.shape[0] *sl_scores.shape[1]))\n",
    "    scores_3D = np.zeros(process_mask.shape)\n",
    "    scores_3D[process_mask] = reshaped_sl_scores\n",
    "\n",
    "\n",
    "    sl_rmse = sl_scores_combined[:,:,1]\n",
    "    reshaped_sl_rmse = np.reshape(sl_rmse,(sl_rmse.shape[0]*sl_rmse.shape[1]))\n",
    "    rmse_3D = np.zeros(process_mask.shape)\n",
    "    rmse_3D[process_mask] = reshaped_sl_rmse\n",
    "\n",
    "    # #save the scores\n",
    "    this_cv_scores.append(scores_3D)\n",
    "    this_cv_rmse.append(rmse_3D)\n",
    "\n",
    "    this_cv_scores = np.moveaxis(this_cv_scores, 0, -1)\n",
    "    this_cv_rmse = np.moveaxis(this_cv_rmse, 0, -1)\n",
    "\n",
    "\n",
    "    # #save the scores to img\n",
    "    this_cv_affine = mask.affine\n",
    "\n",
    "\n",
    "    this_cv_scores_img = new_img_like(mask, this_cv_scores, affine=this_cv_affine)\n",
    "    this_cv_rmse_img = new_img_like(mask, this_cv_rmse, affine=this_cv_affine)\n",
    "\n",
    "    print('saving: iteration' + str(cv_iter))\n",
    "    nib.save(this_cv_scores_img,res_dir + f'cv{cv_iter}_kfold3_searchlight_pearsonr_train_{train_cat}_test_{test_cat}_cv_{job_id}-{job_array_id}.nii.gz')\n",
    "    nib.save(this_cv_rmse_img,res_dir + f'cv{cv_iter}_kfold3_searchlight_rmse_train_{train_cat}_test_{test_cat}_cv_{job_id}-{job_array_id}.nii.gz')\n",
    "\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    iter_line = f\"iteration {cv_iter} finished at {dt_string}\"\n",
    "    print(iter_line)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
